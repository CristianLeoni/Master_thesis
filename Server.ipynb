{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as ds\n",
    "import pandas as pd\n",
    "\n",
    "class Custom_dataset():\n",
    "    def __init__(self,name):\n",
    "        self.data = pd.read_csv('datasets/'+name+'.csv')\n",
    "        self.data= pd.get_dummies(self.data,  prefix = \"one_hot\")\n",
    "        self.DESCR = 'Description'\n",
    "        self.feature_names = self.data.columns\n",
    "        self.data = self.data.to_numpy()\n",
    "    \n",
    "    def data(self):\n",
    "        return self.data.to_numpy()\n",
    "    \n",
    "DATASETS ={\n",
    "    'wine':[ds.load_wine(),[]],\n",
    "    'breast_cancer':[ds.load_breast_cancer(),[]],\n",
    "    #'digits':[ds.load_digits(),[]],\n",
    "    'analysis':[Custom_dataset('classifiers_analysis_results'),[]],\n",
    "    'TEST1':[Custom_dataset('1_informative'),[]],\n",
    "    'TEST2':[Custom_dataset('Three_clusters'),[]],\n",
    "    'TEST3':[Custom_dataset('TEST3_5_informative_50_features'),[]],\n",
    "    'TEST4':[ds.load_iris(),[]]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "METRICS ={\n",
    "    'accuracy_score' : metrics.accuracy_score,\n",
    "    'balanced_accuracy_score' : metrics.balanced_accuracy_score,\n",
    "    #'top_k_accuracy_score' : metrics.top_k_accuracy_score,\n",
    "    'f1_score' : metrics.f1_score,\n",
    "    'roc_auc_score' : metrics.roc_auc_score\n",
    "}\n",
    "\n",
    "import dimensionality_reduction\n",
    "\n",
    "D_R = {\n",
    "    'pca':dimensionality_reduction.Pca,\n",
    "    'tsne':dimensionality_reduction.Tsne,\n",
    "    'Feature_agglomeration':dimensionality_reduction.Feature_agglomeration,\n",
    "}\n",
    "\n",
    "import classifiers\n",
    "\n",
    "CLASSIFIERS = {\n",
    "    #'Tree':classifiers.Tree_classifier,\n",
    "    'Knn':classifiers.Knn_classifier,\n",
    "    'Svc':classifiers.SVC_classifier,\n",
    "    'Decision_tree':classifiers.DecisionTree_classifier,\n",
    "    'Naive_Bayes':classifiers.Naive_Bayes_classifier\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class hashmap():\n",
    "    d = dict({})\n",
    "    i=0\n",
    "    \n",
    "    def add(self,element):\n",
    "        self.i+=1\n",
    "        self.d[self.i]=element\n",
    "        return self.i\n",
    "    \n",
    "    def get(self,i):\n",
    "        print('hello')\n",
    "        return self.d[i]\n",
    "    \n",
    "h = hashmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.layouts import column,row\n",
    "from bokeh.models import ColumnDataSource, CustomJS, Slider, Select\n",
    "from bokeh.plotting import Figure, output_file, save\n",
    "from bokeh.models.widgets import Select,Button\n",
    "from numpy.random import randint\n",
    "from bokeh import events\n",
    "from bokeh.embed import components\n",
    "\n",
    "def get_vis(df,dr,ds):\n",
    "    source = ColumnDataSource(df)\n",
    "    TOOLS = \"box_select,lasso_select\"\n",
    "\n",
    "    plot = Figure(tools = TOOLS,title=dr+' of '+ds,name ='scatter')\n",
    "    plot.scatter('x', 'y', source=source,color='color',line_color='black',line_width=0.3)\n",
    "    select_cluster = Select(title=\"Selected cluster:\", value=\"1\", options=['%s'%(i)for i in range(0,2)])\n",
    "    button = Button(label=\"Next\")\n",
    "    \n",
    "    select_cluster.js_on_change(\"value\", CustomJS(args=dict(source=source),code=\"\"\"\n",
    "    cl = +this.value;\n",
    "    \"\"\"))\n",
    "    source.selected.js_on_change('indices',CustomJS(args=dict(source=source),code=\"\"\"\n",
    "        console.log(\"source changed\");\n",
    "        var data = source.data;\n",
    "        cb_obj.indices.forEach(element =>(data.color[element]=col(cl),data.label[element]=cl))\n",
    "        source.change.emit();\n",
    "    \"\"\"))\n",
    "    button.js_on_event(events.ButtonClick,CustomJS(args=dict(source=source),code=\"\"\"\n",
    "    console.log(\"Button pressed\");\n",
    "    //window.location.href = \"/explanation?dataset=\"+JSON.stringify(source.data['label']).replaceAll('#','%23') \n",
    "    //window.open(\"/explanation?dataset=\"+JSON.stringify(source.data['label']).replaceAll('#','%23'),'_self',source.data)\n",
    "\n",
    "    //openWindowWithPost(source.data['label'])\n",
    "    new_open_window(source.data['label'])\n",
    "\n",
    "    \"\"\"))\n",
    "\n",
    "    return row(column(select_cluster,button),plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:9001/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [07/May/2021 12:56:17] \"\u001b[37mGET /test_scatter_flask?dataset=wine&D_R_params=[]&D_R=pca HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/May/2021 12:56:17] \"\u001b[36mGET /static/Navigation.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [07/May/2021 12:56:43] \"\u001b[37mGET /test_scatter_flask?dataset=wine&D_R_params=[]&D_R=pca HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/May/2021 12:57:42] \"\u001b[37mGET /test_scatter_flask?dataset=wine&D_R_params=[]&D_R=pca HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/May/2021 12:57:42] \"\u001b[36mGET /static/Navigation.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [07/May/2021 12:57:44] \"\u001b[37mPOST /save_test HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/May/2021 12:57:44] \"\u001b[37mGET /classifier_selection?dataset=wine&D_R_params=[]&D_R=pca&selection=1 HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/May/2021 12:57:44] \"\u001b[36mGET /static/Performance.js HTTP/1.1\u001b[0m\" 304 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier_selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/May/2021 12:57:44] \"\u001b[37mGET /get_metrics HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/May/2021 12:57:44] \"\u001b[37mGET /get_classifier_* HTTP/1.1\u001b[0m\" 200 -\n",
      "[2021-05-07 12:57:44,343] ERROR in app: Exception on /get_classifier_performances_-* [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"<ipython-input-13-31fd4b8567c1>\", line 94, in get_classifier_info\n",
      "    defaults = CLASSIFIERS[classifier].get_parameters_default()\n",
      "KeyError: 'performances_-*'\n",
      "127.0.0.1 - - [07/May/2021 12:57:44] \"\u001b[35m\u001b[1mGET /get_classifier_performances_-*?dataset=wine&D_R_params=[]&D_R=pca&selection=1&classifier_params=[]&classifier=Knn HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:  *\n",
      "* selected\n",
      "Classifier:  performances_-*\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-07 12:57:44,513] ERROR in app: Exception on /get_classifier_performances_accuracy_score,balanced_accuracy_score,f1_score,roc_auc_score-* [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"<ipython-input-13-31fd4b8567c1>\", line 118, in get_classifier_perf\n",
      "    trained_classifiers = [[c,CLASSIFIERS[c].get_model().fit(X_train,y_train)] for c in CLASSIFIERS.keys()]\n",
      "  File \"<ipython-input-13-31fd4b8567c1>\", line 118, in <listcomp>\n",
      "    trained_classifiers = [[c,CLASSIFIERS[c].get_model().fit(X_train,y_train)] for c in CLASSIFIERS.keys()]\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 173, in fit\n",
      "    y = self._validate_targets(y)\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 560, in _validate_targets\n",
      "    \" class\" % len(cls))\n",
      "ValueError: The number of classes has to be greater than one; got 1 class\n",
      "127.0.0.1 - - [07/May/2021 12:57:44] \"\u001b[35m\u001b[1mGET /get_classifier_performances_accuracy_score,balanced_accuracy_score,f1_score,roc_auc_score-*?dataset=wine&D_R_params=[]&D_R=pca&selection=1&classifier_params=[]&classifier=Knn HTTP/1.1\u001b[0m\" 500 -\n",
      "[2021-05-07 12:57:44,604] ERROR in app: Exception on /get_classifier_performances_accuracy_score,balanced_accuracy_score,f1_score,roc_auc_score-* [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"<ipython-input-13-31fd4b8567c1>\", line 118, in get_classifier_perf\n",
      "    trained_classifiers = [[c,CLASSIFIERS[c].get_model().fit(X_train,y_train)] for c in CLASSIFIERS.keys()]\n",
      "  File \"<ipython-input-13-31fd4b8567c1>\", line 118, in <listcomp>\n",
      "    trained_classifiers = [[c,CLASSIFIERS[c].get_model().fit(X_train,y_train)] for c in CLASSIFIERS.keys()]\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 173, in fit\n",
      "    y = self._validate_targets(y)\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 560, in _validate_targets\n",
      "    \" class\" % len(cls))\n",
      "ValueError: The number of classes has to be greater than one; got 1 class\n",
      "127.0.0.1 - - [07/May/2021 12:57:44] \"\u001b[35m\u001b[1mGET /get_classifier_performances_accuracy_score,balanced_accuracy_score,f1_score,roc_auc_score-*?dataset=wine&D_R_params=[]&D_R=pca&selection=1&classifier_params=[]&classifier=Knn HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/May/2021 12:57:46] \"\u001b[37mGET /test_scatter_flask?dataset=wine&D_R_params=[]&D_R=pca HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/May/2021 12:57:51] \"\u001b[37mPOST /save_test HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/May/2021 12:57:51] \"\u001b[37mGET /classifier_selection?dataset=wine&D_R_params=[]&D_R=pca&selection=2 HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/May/2021 12:57:52] \"\u001b[37mGET /get_metrics HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier_selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-07 12:57:52,072] ERROR in app: Exception on /get_classifier_performances_-* [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\Cristian\\Miniconda3\\envs\\visualizationEnviroment\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"<ipython-input-13-31fd4b8567c1>\", line 94, in get_classifier_info\n",
      "    defaults = CLASSIFIERS[classifier].get_parameters_default()\n",
      "KeyError: 'performances_-*'\n",
      "127.0.0.1 - - [07/May/2021 12:57:52] \"\u001b[35m\u001b[1mGET /get_classifier_performances_-*?dataset=wine&D_R_params=[]&D_R=pca&selection=2&classifier_params=[]&classifier=Knn HTTP/1.1\u001b[0m\" 500 -\n",
      "127.0.0.1 - - [07/May/2021 12:57:52] \"\u001b[37mGET /get_classifier_* HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:  performances_-*\n",
      "Classifier:  *\n",
      "* selected\n",
      "hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Knn', KNeighborsClassifier()],\n",
       " ['Svc',\n",
       "  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                  ('svc', SVC(probability=True))])],\n",
       " ['Decision_tree', DecisionTreeClassifier()],\n",
       " ['Naive_Bayes', GaussianNB()]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Knn'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[13,  0],\n",
       "       [ 0, 23]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Svc'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[13,  0],\n",
       "       [ 1, 22]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Decision_tree'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[13,  0],\n",
       "       [ 0, 23]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Naive_Bayes'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[13,  0],\n",
       "       [ 3, 20]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Knn</td>\n",
       "      <td>accuracy_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Svc</td>\n",
       "      <td>accuracy_score</td>\n",
       "      <td>0.9722222222222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision_tree</td>\n",
       "      <td>accuracy_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive_Bayes</td>\n",
       "      <td>accuracy_score</td>\n",
       "      <td>0.9166666666666666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knn</td>\n",
       "      <td>balanced_accuracy_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Svc</td>\n",
       "      <td>balanced_accuracy_score</td>\n",
       "      <td>0.9782608695652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision_tree</td>\n",
       "      <td>balanced_accuracy_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive_Bayes</td>\n",
       "      <td>balanced_accuracy_score</td>\n",
       "      <td>0.9347826086956521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Knn</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Svc</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.9777777777777777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Decision_tree</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive_Bayes</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.9302325581395349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Knn</td>\n",
       "      <td>roc_auc_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Svc</td>\n",
       "      <td>roc_auc_score</td>\n",
       "      <td>0.9782608695652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Decision_tree</td>\n",
       "      <td>roc_auc_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Naive_Bayes</td>\n",
       "      <td>roc_auc_score</td>\n",
       "      <td>0.9347826086956521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       classifier                   metric               value\n",
       "0             Knn           accuracy_score                 1.0\n",
       "1             Svc           accuracy_score  0.9722222222222222\n",
       "2   Decision_tree           accuracy_score                 1.0\n",
       "3     Naive_Bayes           accuracy_score  0.9166666666666666\n",
       "4             Knn  balanced_accuracy_score                 1.0\n",
       "5             Svc  balanced_accuracy_score  0.9782608695652174\n",
       "6   Decision_tree  balanced_accuracy_score                 1.0\n",
       "7     Naive_Bayes  balanced_accuracy_score  0.9347826086956521\n",
       "8             Knn                 f1_score                 1.0\n",
       "9             Svc                 f1_score  0.9777777777777777\n",
       "10  Decision_tree                 f1_score                 1.0\n",
       "11    Naive_Bayes                 f1_score  0.9302325581395349\n",
       "12            Knn            roc_auc_score                 1.0\n",
       "13            Svc            roc_auc_score  0.9782608695652174\n",
       "14  Decision_tree            roc_auc_score                 1.0\n",
       "15    Naive_Bayes            roc_auc_score  0.9347826086956521"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/May/2021 12:57:52] \"\u001b[37mGET /get_classifier_performances_accuracy_score,balanced_accuracy_score,f1_score,roc_auc_score-*?dataset=wine&D_R_params=[]&D_R=pca&selection=2&classifier_params=[]&classifier=Knn HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Knn', KNeighborsClassifier()],\n",
       " ['Svc',\n",
       "  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                  ('svc', SVC(probability=True))])],\n",
       " ['Decision_tree', DecisionTreeClassifier()],\n",
       " ['Naive_Bayes', GaussianNB()]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Knn'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[13,  0],\n",
       "       [ 0, 23]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Svc'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[13,  0],\n",
       "       [ 1, 22]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Decision_tree'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[13,  0],\n",
       "       [ 0, 23]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Naive_Bayes'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[13,  0],\n",
       "       [ 3, 20]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Knn</td>\n",
       "      <td>accuracy_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Svc</td>\n",
       "      <td>accuracy_score</td>\n",
       "      <td>0.9722222222222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision_tree</td>\n",
       "      <td>accuracy_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive_Bayes</td>\n",
       "      <td>accuracy_score</td>\n",
       "      <td>0.9166666666666666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knn</td>\n",
       "      <td>balanced_accuracy_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Svc</td>\n",
       "      <td>balanced_accuracy_score</td>\n",
       "      <td>0.9782608695652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision_tree</td>\n",
       "      <td>balanced_accuracy_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive_Bayes</td>\n",
       "      <td>balanced_accuracy_score</td>\n",
       "      <td>0.9347826086956521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Knn</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Svc</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.9777777777777777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Decision_tree</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive_Bayes</td>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.9302325581395349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Knn</td>\n",
       "      <td>roc_auc_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Svc</td>\n",
       "      <td>roc_auc_score</td>\n",
       "      <td>0.9782608695652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Decision_tree</td>\n",
       "      <td>roc_auc_score</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Naive_Bayes</td>\n",
       "      <td>roc_auc_score</td>\n",
       "      <td>0.9347826086956521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       classifier                   metric               value\n",
       "0             Knn           accuracy_score                 1.0\n",
       "1             Svc           accuracy_score  0.9722222222222222\n",
       "2   Decision_tree           accuracy_score                 1.0\n",
       "3     Naive_Bayes           accuracy_score  0.9166666666666666\n",
       "4             Knn  balanced_accuracy_score                 1.0\n",
       "5             Svc  balanced_accuracy_score  0.9782608695652174\n",
       "6   Decision_tree  balanced_accuracy_score                 1.0\n",
       "7     Naive_Bayes  balanced_accuracy_score  0.9347826086956521\n",
       "8             Knn                 f1_score                 1.0\n",
       "9             Svc                 f1_score  0.9777777777777777\n",
       "10  Decision_tree                 f1_score                 1.0\n",
       "11    Naive_Bayes                 f1_score  0.9302325581395349\n",
       "12            Knn            roc_auc_score                 1.0\n",
       "13            Svc            roc_auc_score  0.9782608695652174\n",
       "14  Decision_tree            roc_auc_score                 1.0\n",
       "15    Naive_Bayes            roc_auc_score  0.9347826086956521"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/May/2021 12:57:52] \"\u001b[37mGET /get_classifier_performances_accuracy_score,balanced_accuracy_score,f1_score,roc_auc_score-*?dataset=wine&D_R_params=[]&D_R=pca&selection=2&classifier_params=[]&classifier=Knn HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "#Analyze submodularpick parameter selection for convergence with different explanation count values\n",
    "import classifiers\n",
    "from werkzeug.serving import run_simple\n",
    "from werkzeug.wrappers import Request, Response\n",
    "from flask import Flask,render_template,send_file,make_response,request\n",
    "import json\n",
    "from json import JSONEncoder \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import ensemble, model_selection\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from lime import submodular_pick\n",
    "\n",
    "from my_util import filter_arguments,decode_parameters\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#h = hashmap()\n",
    "app = Flask(__name__)\n",
    "\n",
    "def index_containing_substring(the_list, substring):\n",
    "    for i, s in enumerate(the_list):\n",
    "        if substring in s:\n",
    "              return i\n",
    "    return -1\n",
    "\n",
    "def reduce_dim(df):\n",
    "    pca = PCA()\n",
    "    x_pca = np.array(pca.fit_transform(df))\n",
    "    display(pca.explained_variance_ratio_)\n",
    "    return pd.DataFrame(x_pca[:,0:2],columns=['x','y'])\n",
    "\n",
    "@app.route('/datasets_entries')\n",
    "def dataset_entries():\n",
    "    print(list(DATASETS.keys()))\n",
    "    return json.dumps(list(DATASETS.keys()))\n",
    "\n",
    "@app.route('/dataset_<dataset>_details')\n",
    "def dataset_details(dataset):\n",
    "    return json.dumps(DATASETS[dataset][0].DESCR)\n",
    "\n",
    "@app.route('/get_DR_<dr_method>')\n",
    "def get_dr_info(dr_method):\n",
    "    print('DR: ',dr_method)\n",
    "    if(dr_method == '*'):\n",
    "        return json.dumps(list(D_R.keys()))\n",
    "    \n",
    "    response = []\n",
    "    defaults = D_R[dr_method].get_parameters_default()\n",
    "    for name,value in D_R[dr_method].get_parameters_type().items():\n",
    "        if(isinstance(value,list)):\n",
    "            response.append(['categorical',name,defaults[name],value])\n",
    "        if(isinstance(value,str)):\n",
    "            response.append([value,name,defaults[name]])\n",
    "    return json.dumps(response)\n",
    "\n",
    "@app.route('/test_scatter_flask')\n",
    "def test_scatter():\n",
    "    dr = D_R[request.args.get('D_R')]\n",
    "    initial_df = DATASETS[request.args.get('dataset')][0].data\n",
    "    init_params =decode_parameters(request.args.get('D_R_params'),dr.args)\n",
    "    \n",
    "    df = dr.get_reduced(initial_df,init_params)\n",
    "    if(request.args.get('selection') != None):\n",
    "        color = np.array(['blue','yellow'])\n",
    "        c=h.get(int(request.args.get('selection')))\n",
    "        try:\n",
    "            df['color'] = color[c]\n",
    "            df['label'] = c\n",
    "        except:\n",
    "            df['color'] = 'blue'    \n",
    "            df['label'] = 0\n",
    "    else:\n",
    "        df['color'] = 'blue'    \n",
    "        df['label'] = 0\n",
    "    \n",
    "    layout = get_vis(df,request.args.get('D_R'),request.args.get('dataset'))\n",
    "    script_bok, div_bok = components(layout)\n",
    "    return render_template('test_scatter_flask.html',script_bok=script_bok, div_bok= div_bok)\n",
    "\n",
    "@app.route('/get_classifier_<classifier>')\n",
    "def get_classifier_info(classifier):\n",
    "    print('Classifier: ',classifier)\n",
    "    if(classifier == '*'):\n",
    "        print('* selected')\n",
    "        return json.dumps(list(CLASSIFIERS.keys()))\n",
    "    \n",
    "    response = []\n",
    "    defaults = CLASSIFIERS[classifier].get_parameters_default()\n",
    "    for name,value in CLASSIFIERS[classifier].get_parameters_type().items():\n",
    "        if(isinstance(value,list)):\n",
    "            response.append(['categorical',name,defaults[name],value])\n",
    "        if(isinstance(value,str)):\n",
    "            response.append([value,name,defaults[name]])\n",
    "    return json.dumps(response)\n",
    "\n",
    "@app.route('/get_classifier_performances_<metrics>-<classifiers>')\n",
    "def get_classifier_perf(metrics,classifiers):\n",
    "    metrics = metrics.split(',')\n",
    "    classifiers = classifiers.split(',')\n",
    "    \n",
    "    if(len(metrics)==0 or len(classifiers)==0):\n",
    "        resp = make_response(pd.Dataframe({}).to_csv())\n",
    "        resp.headers[\"Content-Disposition\"] = \"attachment; filename=export.csv\"\n",
    "        resp.headers[\"Content-Type\"] = \"text/csv\"\n",
    "        return resp\n",
    "\n",
    "    df = DATASETS[request.args.get('dataset')][0].data\n",
    "    target = h.get(int(request.args.get('selection')))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.20, random_state=420)    \n",
    "    \n",
    "    if(len(classifiers)==1 and classifiers[0] == '*'):\n",
    "        trained_classifiers = [[c,CLASSIFIERS[c].get_model().fit(X_train,y_train)] for c in CLASSIFIERS.keys()]\n",
    "    elif(len(classifiers)==1):\n",
    "        params = decode_parameters(request.args.get('classifier_params'),CLASSIFIERS[classifiers[0]].args)\n",
    "        print(params)\n",
    "        trained_classifiers = [[classifiers[0],CLASSIFIERS[classifiers[0]].get_model(params).fit(X_train,y_train)]]\n",
    "    else:\n",
    "        trained_classifiers = [[c,CLASSIFIERS[c].get_model().fit(X_train,y_train)] for c in classifiers]\n",
    "\n",
    "    display(trained_classifiers)\n",
    "    [display(c[0],confusion_matrix(y_test, c[1].predict(X_test))) for c in trained_classifiers]\n",
    "    \n",
    "    results=[]\n",
    "    for metric in metrics:\n",
    "        results.extend([[c[0],metric, METRICS[metric](y_test,c[1].predict(X_test))] for c in trained_classifiers ])\n",
    "    results = np.array(results)\n",
    "    df= pd.DataFrame(results,columns = ['classifier','metric','value'])\n",
    "    display(df)\n",
    "    resp = make_response(df.to_csv())\n",
    "    resp.headers[\"Content-Disposition\"] = \"attachment; filename=export.csv\"\n",
    "    resp.headers[\"Content-Type\"] = \"text/csv\"\n",
    "    return resp\n",
    "\n",
    "@app.route('/compare_default_<metrics>-<classifiers>')\n",
    "def compare_default_perfomance(metrics,classifiers):\n",
    "    metrics = metrics.split(',')\n",
    "    c = classifiers.split(',')\n",
    "\n",
    "    \n",
    "    if(len(metrics)==0 or len(classifiers)==0):\n",
    "        resp = make_response(pd.Dataframe({}).to_csv())\n",
    "        resp.headers[\"Content-Disposition\"] = \"attachment; filename=export.csv\"\n",
    "        resp.headers[\"Content-Type\"] = \"text/csv\"\n",
    "        return resp\n",
    "    c=c[0]\n",
    "    df = DATASETS[request.args.get('dataset')][0].data\n",
    "    target = h.get(int(request.args.get('selection')))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.20, random_state=420)    \n",
    "    \n",
    "    \n",
    "    print(CLASSIFIERS[c].args)\n",
    "    print(request.args.get('classifier_params'))\n",
    "    init_params =decode_parameters(request.args.get('classifier_params'),CLASSIFIERS[c].args)\n",
    "    print(init_params,type(init_params))\n",
    "    trained_classifiers=[]\n",
    "    trained_classifiers.append(['default_'+c,CLASSIFIERS[c].get_model().fit(X_train,y_train)])\n",
    "    trained_classifiers.append([c,CLASSIFIERS[c].get_model(init_params).fit(X_train,y_train)])\n",
    "\n",
    "    results=[]\n",
    "    \n",
    "    \n",
    "\n",
    "    for metric in metrics:\n",
    "        [print(c) for c in trained_classifiers]\n",
    "        results.extend([[c[0],metric, METRICS[metric](y_test,c[1].predict(X_test))] for c in trained_classifiers ])\n",
    "    results = np.array(results)\n",
    "    print(results)\n",
    "    df= pd.DataFrame(results,columns = ['classifier','metric','value'])\n",
    "    display(df)\n",
    "    resp = make_response(df.to_csv())\n",
    "    resp.headers[\"Content-Disposition\"] = \"attachment; filename=export.csv\"\n",
    "    resp.headers[\"Content-Type\"] = \"text/csv\"\n",
    "    return resp\n",
    "\n",
    "@app.route('/get_metrics')\n",
    "def get_metrics():\n",
    "    return json.dumps(list(METRICS.keys()))\n",
    "\n",
    "@app.route('/scatter_d3')\n",
    "def test_scatter_d3():\n",
    "    print(request.args)\n",
    "    print(request.args.get('D_R'))\n",
    "\n",
    "    dr = D_R[request.args.get('D_R')]\n",
    "    initial_df = DATASETS[request.args.get('dataset')][0].data\n",
    "    init_params =decode_parameters(request.args.get('D_R_params'),dr.args)\n",
    "    display(request.args.get('D_R_params'))\n",
    "    display(init_params)\n",
    "    df = dr.get_reduced(initial_df,init_params)\n",
    "    sel = int(request.args.get('selection'))\n",
    "    try:\n",
    "        df['selected'] = h.get(sel)\n",
    "    except:\n",
    "        df['selected'] = [0]*initial_df.shape[0]\n",
    "        \n",
    "    resp = make_response(df.to_csv())\n",
    "    resp.headers[\"Content-Disposition\"] = \"attachment; filename=export.csv\"\n",
    "    resp.headers[\"Content-Type\"] = \"text/csv\"\n",
    "    return resp\n",
    "\n",
    "\n",
    "@app.route('/favicon.ico')\n",
    "def icon():\n",
    "    return None\n",
    "\n",
    "@app.route('/')\n",
    "def route():\n",
    "    return render_template('dataset_selection.html')\n",
    "\n",
    "@app.route('/save_test', methods=['GET','POST'])\n",
    "def save_test():\n",
    "    target = list(map(int, request.data.decode(\"utf-8\").split(',')))\n",
    "    i=h.add(target)\n",
    "    response = make_response(str(i), 200)\n",
    "    response.mimetype = \"text/plain\"\n",
    "    return response\n",
    "\n",
    "@app.route('/get_hist_<hist>', methods=['GET','POST'])\n",
    "def get_exp_hist(hist):\n",
    "    df,cat = DATASETS[request.args.get('dataset')]\n",
    "    data = pd.DataFrame(df.data,columns = df.feature_names)\n",
    "    \n",
    "    if('=' in hist):\n",
    "        hist = hist.split('=')[0]\n",
    "        \n",
    "    data = np.array(data[hist])\n",
    "    target = np.array(h.get(int(request.args.get('selection'))))\n",
    "\n",
    "    df = pd.DataFrame(data[target==0])\n",
    "    df.hist(bins=10)\n",
    "    df = pd.DataFrame(data[target==1])\n",
    "    df.hist(bins=10)\n",
    "    plt.show()\n",
    "\n",
    "    resp = {'0':list(data[target==0]),'1':list(data[target==1])}\n",
    "    return json.dumps(resp)\n",
    "\n",
    "@app.route('/corr_matrix')\n",
    "def corr_matrix():\n",
    "    df,cat = DATASETS[request.args.get('dataset')]\n",
    "    data = pd.DataFrame(df.data,columns = df.feature_names)\n",
    "    data = data.corr().round(2)\n",
    "    data=data.set_index(df.feature_names[0],drop=True)\n",
    "    resp = make_response(data.to_csv())\n",
    "    resp.headers[\"Content-Disposition\"] = \"attachment; filename=export.csv\"\n",
    "    resp.headers[\"Content-Type\"] = \"text/csv\"\n",
    "    return resp\n",
    "\n",
    "\n",
    "@app.route('/elaborate_dataset_id_<my_id>')\n",
    "def evaluate_id(my_id):\n",
    "    df,cat = DATASETS[request.args.get('dataset')]\n",
    "    data = df.data\n",
    "    target = h.get(int(my_id))\n",
    "    \n",
    "    rf = CLASSIFIERS[request.args.get('classifier')]\n",
    "    init_params =decode_parameters(request.args.get('classifier_params'),rf.args)\n",
    "    print(init_params)\n",
    "    rf= rf.get_model(init_params)\n",
    "\n",
    "    train, test, labels_train, labels_test = train_test_split(data,target,train_size=0.80, test_size=0.20)\n",
    "    rf.fit(train, labels_train)\n",
    "\n",
    "    explainer = LimeTabularExplainer(train, feature_names=df.feature_names, class_names=['target'], \n",
    "                                     categorical_features=cat, verbose=False, mode='classification',\n",
    "                                     discretize_continuous=False)\n",
    "\n",
    "    predict_fn = lambda x: rf.predict_proba(x)\n",
    "\n",
    "    #Single explanation test\n",
    "    exp = explainer.explain_instance(test[1,:], predict_fn, num_features=2)\n",
    "    exp.show_in_notebook(show_all=False)\n",
    "    print(exp.available_labels())\n",
    "    \n",
    "    sp_obj = submodular_pick.SubmodularPick(data = train,explainer=explainer,\n",
    "                                                predict_fn=rf.predict_proba ,sample_size=100,\n",
    "                                                #num_features=10,\n",
    "                                                num_exps_desired=30,top_labels=2)\n",
    "    #plt.figure(figsize=(13,13))\n",
    "    #tree.plot_tree(rf,filled=True)\n",
    "    #plt.show()\n",
    "    #print(export_text(rf, feature_names=df.feature_names))\n",
    "    display(confusion_matrix(labels_test, rf.predict(test)))\n",
    "    #[exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];\n",
    "    \n",
    "    W=pd.DataFrame([dict(this.as_list()) for this in sp_obj.explanations])\n",
    "    display(W)\n",
    "    #Sort matrix\n",
    "    W=W.fillna(0)#W.mean()\n",
    "    \n",
    "    #Order by abs mean\n",
    "    order = W.apply(lambda c: c.abs().sum(),axis = 0)\n",
    "    ordered = np.argsort(-order)\n",
    "    display(ordered)\n",
    "    cols = [W.columns[i] for i in ordered]\n",
    "    W = W[cols]\n",
    "    #Reduce W if too many columns\n",
    "    if(W.shape[1]>21):\n",
    "        W=W.iloc[:, 0:21]\n",
    "    W =  W.set_index(W.columns[0])\n",
    "    \n",
    "    resp = make_response(W.to_csv())\n",
    "    resp.headers[\"Content-Disposition\"] = \"attachment; filename=export.csv\"\n",
    "    resp.headers[\"Content-Type\"] = \"text/csv\"\n",
    "    return resp\n",
    "\n",
    "@app.route('/<page>', methods=['GET', 'POST'])\n",
    "def rout(page):\n",
    "    print(page)\n",
    "    return render_template(page+'.html')#,params=request.form['json']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_simple('localhost', 9001, app,reloader_interval = 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1dd4e058928a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-21e41eae668c>\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hello'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhashmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "np.array(h.get(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(7,7))\n",
    "tree.plot_tree(rf,filled=True)\n",
    "plt.show()\n",
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "print(export_text(rf, feature_names=['sepal length','sepal width','petal length']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "explainer = LimeTabularExplainer(train, feature_names=[ 'sepal length','sepal width','petal length'], class_names=['petal width'], categorical_features=[], \n",
    "                                 verbose=False, mode='regression',discretize_continuous=False)\n",
    "exp = explainer.explain_instance(test.to_numpy()[i], rf.predict, num_features=5)\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.get_dataset('test').corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YE OLD CODE\n",
    "\"\"\"@app.route('/elaborate_dataset', methods=['GET', 'POST'])\n",
    "def evaluate():\n",
    "    df = d.get_dataset('test')\n",
    "    target = json.loads(request.form['json'])\n",
    "    target = list(map(int, target['selected'].split(',')))\n",
    "    print(h.add(target))\n",
    "\n",
    "    train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(df.drop(['petal width'], axis=1),\n",
    "                                                                                      target,\n",
    "                                                                                      train_size=0.80, test_size=0.20)\n",
    "    rf=c.get_classifier('tree').fit(train, labels_train);\n",
    "    display(train)\n",
    "    iris_features =['sepal length', 'sepal width', 'petal length', 'petal width', 'species__Iris-setosa', 'species__Iris-versicolor','species__Iris-virginica']\n",
    "    explainer = LimeTabularExplainer(train, feature_names=iris_features, class_names=['target'], categorical_features=[], verbose=False, mode='regression',discretize_continuous=False)\n",
    "    display(explainer)\n",
    "    sp_obj = submodular_pick.SubmodularPick(explainer, train.to_numpy(), rf.predict,sample_size=10\n",
    "                                            , num_features=4, num_exps_desired=10)\n",
    "    [exp.as_pyplot_figure() for exp in sp_obj.sp_explanations];\n",
    "    plt.show()\n",
    "    W=pd.DataFrame([dict(this.as_list()) for this in sp_obj.explanations])\n",
    "    W =  W.set_index('sepal length')\n",
    "    display(W)\n",
    "    resp = make_response(W.to_csv())\n",
    "    resp.headers[\"Content-Disposition\"] = \"attachment; filename=export.csv\"\n",
    "    resp.headers[\"Content-Type\"] = \"text/csv\"\n",
    "    return resp\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test(my_id =1):\n",
    "    df,cat = d.get_data_cat('test')\n",
    "    target = h.get(1)\n",
    "    train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(df,target,train_size=0.80, test_size=0.20)\n",
    "    rf=c.get_classifier('tree').fit(train, labels_train);\n",
    "\n",
    "    iris_features =['sepal length', 'sepal width', 'petal length', 'petal width','one_hot_Iris-setosa', 'one_hot_Iris-versicolor', 'one_hot_Iris-virginica']\n",
    "\n",
    "    #reshaped_decision = np.reshape(decision, (len(decision),1))\n",
    "    explainer = LimeTabularExplainer(train, feature_names=iris_features, class_names=['non_selected','selected'], \n",
    "                                     categorical_features=[], verbose=False, mode='classification',\n",
    "                                     discretize_continuous=False)\n",
    "\n",
    "    predict_fn = lambda x: rf.predict_proba(x)#\n",
    "    exp = explainer.explain_instance(test.iloc[1], predict_fn, num_features=5)\n",
    "    exp.show_in_notebook(show_all=False)\n",
    "    print(exp.available_labels())\n",
    "\n",
    "    sp_obj = submodular_pick.SubmodularPick(data = train.to_numpy(),explainer=explainer, \n",
    "                                            predict_fn=rf.predict_proba ,sample_size=10,num_features=5,\n",
    "                                            num_exps_desired=10,top_labels=3)\n",
    "    display(sp_obj.sp_explanations[0].as_list(),dir(sp_obj.sp_explanations[0]))\n",
    "    W=pd.DataFrame([dict(this.as_list()) for this in sp_obj.explanations])\n",
    "    #display(W)\n",
    "    #[exp.as_pyplot_figure() for exp in sp_obj.sp_explanations]\n",
    "    df=pd.DataFrame({})\n",
    "    \n",
    "    for this_label in range(2):\n",
    "        dfl=[]\n",
    "        for i,exp in enumerate(sp_obj.sp_explanations):\n",
    "            l=exp.as_list(label=this_label)\n",
    "            l.append((\"exp number\",i))\n",
    "            dfl.append(dict(l))\n",
    "        dftest=pd.DataFrame(dfl)\n",
    "        df=df.append(pd.DataFrame(dfl,index=[this_label for i in range(len(sp_obj.sp_explanations))]))\n",
    "    display(df)\n",
    "    [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = DATASETS['iris'][0]\n",
    "temp = pd.DataFrame(d.data,columns =d.feature_names)\n",
    "temp2= np.array(h.get(6))\n",
    "\n",
    "def plt_diff(col):\n",
    "    db = temp[col]\n",
    "    x = db[np.where(temp2 == 0)[0]].values\n",
    "    y = db[np.where(temp2 == 1)[0]].values\n",
    "    bins = np.linspace(0, 10, 100)\n",
    "\n",
    "    plt.hist(x, bins, alpha=0.5, label='0')\n",
    "    plt.hist(y, bins, alpha=0.5, label='1')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(col)\n",
    "    plt.show()\n",
    "    \n",
    "[plt_diff(col) for col in DATASETS['iris'][0].feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1,min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0}\".replace('=',':')\n",
    "\"n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None\".replace('=',':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "#DATASETS['imp_3_red_2'][0].feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = DATASETS['imp_4'][0]\n",
    "\n",
    "dr= D_R['pca']\n",
    "temp = dr.get_reduced(d.data,{})\n",
    "temp2= np.array(h.get(2))\n",
    "colors = ['blue','yellow']\n",
    "colors2 = ['blue','yellow','green']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20,10))\n",
    "\n",
    "ax2.scatter(temp['x'],temp['y'],c = [colors2[c] for c in temp2])\n",
    "\n",
    "ax1.scatter(temp['x'],temp['y'],color = [colors2[int(c)] for c in target])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,cat = DATASETS['test']\n",
    "data = pd.DataFrame(df.data,columns = df.feature_names)\n",
    "data = data.corr().round(2)\n",
    "data=data.set_index(df.feature_names[0],drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3]\n",
    "a.append([4,5,6])\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
